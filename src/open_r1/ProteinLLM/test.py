import os
import sys
import torch
import json
from datasets import Dataset
from transformers import TrainingArguments

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/x-cli32/chaohan/projects/open-r1/src')

from open_r1.ProteinLLM.ProteinLLMModel import ProteinLLMModel, ProteinLLMConfig
from open_r1.ProteinLLM.ProteinLLMProcessor import ProteinLLMProcessor

# ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä½ çš„çœŸå®æ•°æ®
def load_real_data():
    """åŠ è½½ä½ çš„å®é™…æ•°æ®"""
    data_path = "/home/x-cli32/chaohan/projects/open-r1/src/open_r1/ProteinLLM/data.jsonl"
    
    samples = []
    with open(data_path, 'r') as f:
        for line in f:
            samples.append(json.loads(line))
    
    print(f"Loaded {len(samples)} real samples")
    return samples

def test_processor_separately():
    """å•ç‹¬æµ‹è¯•ProcessoråŠŸèƒ½"""
    print("=== Testing Processor Separately ===")
    
    from transformers import AutoTokenizer
    
    # åŠ è½½tokenizers
    text_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Math-1.5B", trust_remote_code=True)
    protein_tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t12_35M_UR50D")
    
    # æ·»åŠ è›‹ç™½è´¨tokenåˆ°æ–‡æœ¬tokenizer
    text_tokenizer.add_special_tokens({"additional_special_tokens": ["<|protein_pad|>"]})
    
    # åˆ›å»ºprocessor
    processor = ProteinLLMProcessor(
        tokenizer=text_tokenizer,
        protein_tokenizer=protein_tokenizer
    )
    
    # æµ‹è¯•æ ·æœ¬
    test_messages = [
        {
            "role": "user", 
            "content": "Does this protein sequence include a signal peptide?\n\nSequence:\n<|protein_pad|>"
        }
    ]
    
    # ä½¿ç”¨chat_templateè½¬æ¢
    formatted_text = text_tokenizer.apply_chat_template(
        test_messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    print(f"Formatted text: {formatted_text[:200]}...")
    
    # æµ‹è¯•processor
    result = processor(
        batch_protein_sequences=[["MKIIFLVLMMILSEVYSDRDGYPVHDGTNCKYSCDIREKWEYCTPLCKRRNAKTGYCYAFACWCIGLPDE"]],
        text=[formatted_text],
        max_length_text=512,
        max_length_protein=100
    )
    
    print(f"Processor output keys: {result.keys()}")
    print(f"Input IDs shape: {result['input_ids'].shape}")
    print(f"Protein tokenized shape: {result['protein_tokenized']['input_ids'].shape}")
    print(f"Batch idx map: {result['batch_idx_map']}")
    
    # ğŸ”§ éªŒè¯ESM tokenæ•°é‡
    protein_tokens = result['protein_tokenized']['input_ids'][0]
    attention_mask = result['protein_tokenized']['attention_mask'][0]
    print(f"Protein total tokens: {attention_mask.sum().item()}")
    print(f"Protein tokens (first 10): {protein_tokens[:10].tolist()}")
    
    return processor

def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print("=== Testing Model Creation ===")
    
    config = ProteinLLMConfig(
        text_model_name="Qwen/Qwen2.5-Math-1.5B",
        protein_model_name="facebook/esm2_t12_35M_UR50D",
        text_model_finetune=True,
        protein_model_finetune=False  # å†»ç»“è›‹ç™½è´¨æ¨¡å‹
    )
    
    model = ProteinLLMModel(config=config)
    
    print(f"Text model: {model.text_model_name}")
    print(f"Protein model: {model.protein_model_name}")
    print(f"Protein token ID: {model.protein_token_id}")
    print(f"Text vocab size: {len(model.text_tokenizer)}")
    
    return model

def test_forward_pass(model, processor):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("=== Testing Forward Pass ===")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_data = load_real_data()[:2]  # ä½¿ç”¨å‰2ä¸ªçœŸå®æ ·æœ¬
    
    # è½¬æ¢ä¸ºSFTTraineræœŸæœ›çš„æ ¼å¼
    formatted_texts = []
    protein_sequences = []
    
    for sample in test_data:
        # ä½¿ç”¨chat_templateè½¬æ¢messages
        text = model.text_tokenizer.apply_chat_template(
            sample["messages"], 
            tokenize=False, 
            add_generation_prompt=False
        )
        formatted_texts.append(text)
        protein_sequences.append(sample["protein_sequence"])
    
    print(f"Sample formatted text: {formatted_texts[0][:200]}...")
    
    # ä½¿ç”¨processorå¤„ç†
    batch_inputs = processor(
        batch_protein_sequences=protein_sequences,
        text=formatted_texts,
        max_length_text=512,
        max_length_protein=100,
        return_tensors="pt"
    )
    
    print(f"Batch inputs keys: {batch_inputs.keys()}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        outputs = model(**batch_inputs)
    
    print(f"Model outputs keys: {outputs.keys()}")
    print(f"Logits shape: {outputs.logits.shape}")
    
    return batch_inputs, outputs

def test_sft_trainer():
    """æµ‹è¯•SFTTraineré›†æˆ"""
    print("=== Testing SFTTrainer Integration ===")
    
    # ğŸ”§ ä½¿ç”¨ä½ çš„çœŸå®æ•°æ®
    real_data = load_real_data()
    dataset = Dataset.from_list(real_data)
    
    print(f"Dataset size: {len(dataset)}")
    print(f"Sample keys: {dataset[0].keys()}")
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨å°æ¨¡å‹ï¼‰
    config = ProteinLLMConfig(
        text_model_name="Qwen/Qwen2.5-Math-1.5B",
        protein_model_name="facebook/esm2_t12_35M_UR50D"
    )
    model = ProteinLLMModel(config=config)
    
    # è®­ç»ƒå‚æ•°ï¼ˆæç®€æµ‹è¯•ï¼‰
    training_args = TrainingArguments(
        output_dir="./test_output",
        num_train_epochs=1,
        per_device_train_batch_size=2,  # å°batch size
        learning_rate=1e-5,
        logging_steps=1,
        save_steps=100,
        max_steps=3,  # åªè·‘3æ­¥éªŒè¯
        dataloader_drop_last=False,
        remove_unused_columns=False,  # ä¿ç•™protein_sequenceåˆ—
        gradient_checkpointing=True,  # èŠ‚çœå†…å­˜
        fp16=True,  # ä½¿ç”¨æ··åˆç²¾åº¦
    )
    
    # ğŸ”§ ä¿®å¤ï¼šå¯¼å…¥æ­£ç¡®çš„SFTTrainer
    try:
        # å°è¯•ä»trlå¯¼å…¥ï¼ˆå¦‚æœå®‰è£…äº†TRLï¼‰
        from trl import SFTTrainer
        print("Using TRL SFTTrainer")
    except ImportError:
        # å¦‚æœæ²¡æœ‰TRLï¼Œä½¿ç”¨transformersçš„ç®€å•ç‰ˆæœ¬
        print("TRL not found, creating simple trainer")
        from transformers import Trainer
        
        class SimpleSFTTrainer(Trainer):
            def __init__(self, processing_class=None, **kwargs):
                super().__init__(**kwargs)
                self.processing_class = processing_class
            
            def _prepare_dataset(self, dataset, tokenizer=None, packing=None, **kwargs):
                def process_sample(examples):
                    # è½¬æ¢messagesä¸ºæ–‡æœ¬
                    texts = []
                    for messages in examples["messages"]:
                        text = self.tokenizer.apply_chat_template(
                            messages, tokenize=False, add_generation_prompt=False
                        )
                        texts.append(text)
                    
                    # ä½¿ç”¨processing_classå¤„ç†
                    processed = self.processing_class(
                        batch_protein_sequences=examples["protein_sequence"],
                        text=texts
                    )
                    
                    # æ·»åŠ labelsï¼ˆSFTéœ€è¦ï¼‰
                    processed["labels"] = processed["input_ids"].clone()
                    
                    return processed
                
                return dataset.map(process_sample, batched=True, remove_columns=dataset.column_names)
        
        SFTTrainer = SimpleSFTTrainer
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        processing_class=model.processor,
        tokenizer=model.text_tokenizer,
    )
    
    print("Starting SFT training test...")
    trainer.train()
    print("SFT training test completed!")

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("ğŸ§¬ Starting ProteinLLM Open-R1 Integration Test")
    print("=" * 60)
    
    try:
        # Step 1: æµ‹è¯•Processor
        processor = test_processor_separately()
        print("âœ… Processor test passed\n")
        
        # Step 2: æµ‹è¯•æ¨¡å‹åˆ›å»º  
        model = test_model_creation()
        print("âœ… Model creation test passed\n")
        
        # Step 3: æµ‹è¯•å‰å‘ä¼ æ’­
        batch_inputs, outputs = test_forward_pass(model, processor)
        print("âœ… Forward pass test passed\n")
        
        # Step 4: æµ‹è¯•SFTTraineré›†æˆ
        test_sft_trainer()
        print("âœ… SFTTrainer integration test passed\n")
        
        print("ğŸ‰ All tests passed! Ready for full training.")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()