"""
æµ‹è¯•é‡æ–°è®¾è®¡çš„åŒå±‚æ¶æ„ï¼šProcessor + DataCollator
"""
import os
import sys
import torch
import json
from datasets import Dataset
from transformers import AutoTokenizer, TrainingArguments

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/cl426/data/open-r1/src')

from open_r1.ProteinLLM.ProteinLLMModel import ProteinLLMModel, ProteinLLMConfig
from open_r1.ProteinLLM.ProteinLLMProcessor import ProteinLLMProcessor
from open_r1.ProteinLLM.ProteinLLMDataCollator import ProteinLLMDataCollator

def load_test_data():
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    data_path = "/home/cl426/data/open-r1/src/open_r1/ProteinLLM/data.jsonl"
    
    samples = []
    with open(data_path, 'r') as f:
        for line in f:
            samples.append(json.loads(line))
    
    print(f"Loaded {len(samples)} test samples")
    print(f"Sample protein_sequence type: {type(samples[0]['protein_sequence'])}")
    print(f"Sample protein_sequence length: {len(samples[0]['protein_sequence'])}")
    return samples[:3]  # åªç”¨å‰3ä¸ªæ ·æœ¬æµ‹è¯•

def test_processor_alone():
    """æµ‹è¯•Processorå•ç‹¬å·¥ä½œ"""
    print("=== æµ‹è¯•Processorå•ç‹¬å·¥ä½œ ===")
    
    # 1. å‡†å¤‡tokenizers
    text_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Math-1.5B", trust_remote_code=True)
    protein_tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t12_35M_UR50D")
    
    # 2. æ·»åŠ ç‰¹æ®Štoken
    text_tokenizer.add_special_tokens({"additional_special_tokens": ["<|protein_pad|>"]})
    
    # 3. åˆ›å»ºProcessor
    processor = ProteinLLMProcessor(
        tokenizer=text_tokenizer,
        protein_tokenizer=protein_tokenizer
    )
    
    # 4. å‡†å¤‡æµ‹è¯•æ•°æ®
    test_data = load_test_data()
    
    # æå–æ–‡æœ¬å’Œè›‹ç™½è´¨åºåˆ—
    texts = []
    protein_sequences = []
    
    for sample in test_data:
        # ä½¿ç”¨chat_templateå¤„ç†messages
        text = text_tokenizer.apply_chat_template(
            sample["messages"], 
            tokenize=False, 
            add_generation_prompt=False
        )
        texts.append(text)
        protein_sequences.append(sample["protein_sequence"])  # ç°åœ¨æ˜¯å­—ç¬¦ä¸²
    
    print(f"Sample text: {texts[0][:100]}...")
    print(f"Sample protein: {protein_sequences[0][:20]}...")
    
    # 5. æµ‹è¯•Processor
    try:
        result = processor(
            text=texts,
            protein_sequence=protein_sequences,
            max_length_text=512,
            max_length_protein=100,
            return_tensors="pt"
        )
        
        print(f"âœ… ProcessoræˆåŠŸï¼")
        print(f"Result keys: {result.keys()}")
        print(f"Text shape: {result['input_ids'].shape}")
        print(f"Protein shape: {result['protein_tokenized']['input_ids'].shape}")
        print(f"Batch idx map: {result['batch_idx_map']}")
        
        return processor, result
        
    except Exception as e:
        print(f"âŒ Processorå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_datacollator_with_processor():
    """æµ‹è¯•DataCollatorä¸Processorçš„åä½œ"""
    print("\\n=== æµ‹è¯•DataCollatorä¸Processoråä½œ ===")
    
    # 1. åˆ›å»ºæ¨¡å‹ï¼ˆä¸ºäº†è·å–processorï¼‰
    config = ProteinLLMConfig(
        text_model_name="Qwen/Qwen2.5-Math-1.5B",
        protein_model_name="facebook/esm2_t12_35M_UR50D",
        text_model_finetune=True,
        protein_model_finetune=False
    )
    
    try:
        model = ProteinLLMModel(config=config)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # 2. åˆ›å»ºDataCollator
        data_collator = ProteinLLMDataCollator(
            processor=model.processor,
            max_length_text=512,
            max_length_protein=100
        )
        
        # 3. å‡†å¤‡æ•°æ®
        test_data = load_test_data()
        dataset = Dataset.from_list(test_data)
        
        # 4. æµ‹è¯•DataCollator
        batch_features = [dataset[i] for i in range(min(2, len(dataset)))]
        
        batch = data_collator(batch_features)
        
        print(f"âœ… DataCollatoræˆåŠŸï¼")
        print(f"Batch keys: {batch.keys()}")
        print(f"Input IDs shape: {batch['input_ids'].shape}")
        if batch.get('protein_tokenized'):
            print(f"Protein shape: {batch['protein_tokenized']['input_ids'].shape}")
        print(f"Labels shape: {batch['labels'].shape}")
        
        return data_collator, batch
        
    except Exception as e:
        print(f"âŒ DataCollatoræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_sft_trainer_integration():
    """æµ‹è¯•SFTTraineré›†æˆ - çœŸå®ç‰ˆæœ¬"""
    print("\\n=== æµ‹è¯•SFTTraineré›†æˆ - çœŸå®ç‰ˆæœ¬ ===")
    
    try:
        from trl import SFTTrainer
        print("âœ… æˆåŠŸå¯¼å…¥SFTTrainer")
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥SFTTrainerï¼Œè¯·å®‰è£…TRLåº“")
        return None, None
    
    # 1. åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºåŒæ¨¡æ€æ¨¡å‹...")
    config = ProteinLLMConfig(
        text_model_name="Qwen/Qwen2.5-Math-1.5B",
        protein_model_name="facebook/esm2_t12_35M_UR50D",
        text_model_finetune=True,
        protein_model_finetune=False
    )
    
    try:
        model = ProteinLLMModel(config=config)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # 2. åˆ›å»ºDataCollator
        data_collator = ProteinLLMDataCollator(
            processor=model.processor,
            max_length_text=512,
            max_length_protein=100
        )
        print(f"âœ… DataCollatoråˆ›å»ºæˆåŠŸ")
        
        # 3. å‡†å¤‡æ•°æ®é›†
        test_data = load_test_data()
        dataset = Dataset.from_list(test_data)
        print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")
        
        # 4. é…ç½®è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir="./test_sft_output",
            max_steps=5,  # åªè®­ç»ƒ5æ­¥æ¥æµ‹è¯•
            per_device_train_batch_size=1,  # å°batch sizeé¿å…å†…å­˜é—®é¢˜
            learning_rate=1e-5,
            logging_steps=1,
            save_steps=10,
            gradient_checkpointing=False,
            fp16=False,  # é¿å…æ•°å€¼é—®é¢˜
            bf16=False,
            remove_unused_columns=False,  # ä¿ç•™protein_sequenceåˆ—
            dataloader_drop_last=False,
            eval_strategy="no",  # ä¸è¿›è¡Œè¯„ä¼°
            save_strategy="no",   # ä¸ä¿å­˜checkpoint
        )
        print(f"âœ… è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ")
        
        # 5. åˆ›å»ºSFTTrainer - æ­£ç¡®çš„åŒæ¨¡æ€é…ç½®
        print("åˆ›å»ºSFTTrainer...")
        trainer = SFTTrainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,        # ğŸ”§ å…³é”®ï¼šè®©DataCollatorå¤„ç†æ‰€æœ‰æ•°æ®
            # ğŸ”§ é‡è¦ï¼šå®Œå…¨ç¦ç”¨SFTTrainerçš„æ•°æ®é¢„å¤„ç†
            formatting_func=None,              # ä¸ä½¿ç”¨formattingå‡½æ•° - è®©DataCollatorå…¨æƒå¤„ç†
            # ä¸ä¼ å…¥processing_classï¼Œé¿å…SFTTrainerè°ƒç”¨Processor
        )
        print(f"âœ… SFTTraineråˆ›å»ºæˆåŠŸ")
        
        # 6. è¿è¡Œè®­ç»ƒæµ‹è¯•
        print("å¼€å§‹è®­ç»ƒæµ‹è¯•ï¼ˆ5æ­¥ï¼‰...")
        train_result = trainer.train()
        
        print(f"âœ… SFTTrainerè®­ç»ƒæµ‹è¯•æˆåŠŸï¼")
        print(f"è®­ç»ƒæŒ‡æ ‡: {train_result.metrics}")
        
        return trainer, train_result
        
    except Exception as e:
        print(f"âŒ SFTTraineré›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("ğŸ§¬ æµ‹è¯•é‡æ–°è®¾è®¡çš„åŒå±‚æ¶æ„")
    print("=" * 60)
    
    # æµ‹è¯•Processor
    processor, processor_result = test_processor_alone()
    if processor_result is not None:
        print("âœ… Processoræµ‹è¯•é€šè¿‡\\n")
    
    # æµ‹è¯•DataCollator
    data_collator, batch = test_datacollator_with_processor()
    if batch is not None:
        print("âœ… DataCollatoræµ‹è¯•é€šè¿‡\\n")
    
    # æµ‹è¯•SFTTraineré›†æˆ - çœŸå®ç‰ˆæœ¬
    trainer, train_result = test_sft_trainer_integration()
    if train_result is not None:
        print("âœ… SFTTraineré›†æˆæµ‹è¯•é€šè¿‡\\n")
    
    print("\\nğŸ‰ å®Œæ•´æ¶æ„æµ‹è¯•å®Œæˆï¼")
    
    # æ€»ç»“æŠ¥å‘Š
    print("\\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"- Processor: {'âœ… é€šè¿‡' if processor_result is not None else 'âŒ å¤±è´¥'}")
    print(f"- DataCollator: {'âœ… é€šè¿‡' if batch is not None else 'âŒ å¤±è´¥'}")
    print(f"- SFTTrainer: {'âœ… é€šè¿‡' if train_result is not None else 'âŒ å¤±è´¥'}")
    
    if all([processor_result is not None, batch is not None, train_result is not None]):
        print("\\nğŸ† æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åŒæ¨¡æ€æ¶æ„å‡†å¤‡å°±ç»ªï¼")

if __name__ == "__main__":
    main()
