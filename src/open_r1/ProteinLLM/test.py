"""
æµ‹è¯•é‡æ–°è®¾è®¡çš„åŒå±‚æ¶æ„ï¼šProcessor + DataCollator
"""
import os
import sys
import torch
import json
import glob
from datasets import Dataset
from transformers import AutoTokenizer, TrainingArguments
from torch.utils.data import DataLoader
from trl import SFTTrainer

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/x-cli32/chaohan/projects/open-r1/src')

import glob
from huggingface_hub import snapshot_download
from huggingface_hub.utils import LocalEntryNotFoundError

# æ·»åŠ ï¼šæ›´ç¨³å¥çš„ç¼“å­˜æ£€æŸ¥
def _resolve_cached_snapshot(repo_id: str, cache_dirs=None) -> str | None:
    """
    è¿”å›æœ¬åœ°ç¼“å­˜çš„ snapshot ç»å¯¹è·¯å¾„ï¼›è‹¥ä¸å­˜åœ¨åˆ™è¿”å› Noneã€‚
    ä¼˜å…ˆä½¿ç”¨ HF_HOMEï¼Œå…¶æ¬¡ä½¿ç”¨ç”¨æˆ· home ä¸‹é»˜è®¤è·¯å¾„ã€‚
    """
    if cache_dirs is None:
        cache_dirs = [
            os.getenv("HF_HOME", os.path.expanduser("~/.cache/huggingface")),
            "/home/x-cli32/chaohan/huggingface",  # ä½ çš„è‡ªå®šä¹‰ç¼“å­˜è·¯å¾„ï¼ˆå¯æ”¾åé¢ä½œä¸ºå…œåº•ï¼‰
        ]
    for cache_dir in cache_dirs:
        try:
            path = snapshot_download(
                repo_id=repo_id,
                cache_dir=cache_dir,
                local_files_only=True,   # åªæŸ¥æœ¬åœ°ï¼Œä¸è§¦ç½‘
                resume_download=True,
            )
            if os.path.isdir(path):
                return path
        except LocalEntryNotFoundError:
            continue
        except Exception:
            continue
    return None

def check_model_cache(model_name: str, model_type: str = "any") -> bool:
    """
    æ›´ç¨³å¥çš„ç¼“å­˜æ£€æŸ¥ï¼š
      - tokenizer: æ¥å— tokenizer.json æˆ– vocab.txt/tokenizer.model ç­‰å½¢å¼
      - model: æ¥å— model.safetensors / model-*.safetensors / pytorch_model.bin
      - config: æ¥å— config.json
      - any: åªè¦èƒ½è§£æåˆ° snapshot å³è®¤ä¸ºå·²ç¼“å­˜
    """
    snapshot_dir = _resolve_cached_snapshot(model_name)
    if snapshot_dir is None:
        return False

    if model_type == "any":
        return True

    # å…è®¸çš„å¤šç§å‘½åä¸æ ¼å¼
    patterns = {
        "tokenizer": [
            "tokenizer.json",
            "tokenizer_config.json",
            "vocab.txt",
            "vocab.json",
            "merges.txt",
            "tokenizer.model",  # sentencepiece
        ],
        "model": [
            "model.safetensors",
            "model-*.safetensors",
            "pytorch_model.bin",
            "tf_model.h5",
            "flax_model.msgpack",
        ],
        "config": ["config.json"],
    }
    required_any = patterns.get(model_type, ["config.json"])
    for pat in required_any:
        if glob.glob(os.path.join(snapshot_dir, pat)):
            return True
    return False

def print_cache_status():
    """æ‰“å°ç¼“å­˜çŠ¶æ€ä¿¡æ¯ï¼ˆä½¿ç”¨æ–°å®ç°ï¼‰"""
    cache_dir = os.getenv("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
    print(f"ğŸ” HuggingFaceç¼“å­˜ç›®å½•: {cache_dir}")

    models_to_check = [
        "Qwen/Qwen3-1.7B",
        "facebook/esm2_t12_35M_UR50D",
    ]

    print("\nğŸ“¦ ç¼“å­˜çŠ¶æ€æ£€æŸ¥:")
    for model in models_to_check:
        any_cached = check_model_cache(model, "any")
        tok_cached = check_model_cache(model, "tokenizer")
        mdl_cached = check_model_cache(model, "model")
        cfg_cached = check_model_cache(model, "config")

        status = "âœ… å·²ç¼“å­˜" if any_cached else "âŒ æœªç¼“å­˜"
        print(f"  {model}: {status}")
        if any_cached:
            print(f"    - Tokenizer files: {'âœ…' if tok_cached else 'âŒ'}")
            print(f"    - Model weights:   {'âœ…' if mdl_cached else 'âŒ'}")
            print(f"    - Config:          {'âœ…' if cfg_cached else 'âŒ'}")
    print()

from open_r1.ProteinLLM.ProteinLLMModel import ProteinLLMModel, ProteinLLMConfig
from open_r1.ProteinLLM.ProteinLLMProcessor import ProteinLLMProcessor
from open_r1.ProteinLLM.ProteinLLMDataCollator import ProteinLLMDataCollator

def load_test_data():
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    data_path = "/home/x-cli32/chaohan/projects/open-r1/src/open_r1/ProteinLLM/data.jsonl"
    
    samples = []
    with open(data_path, 'r') as f:
        for line in f:
            samples.append(json.loads(line))
    
    print(f"Loaded {len(samples)} test samples")
    print(f"Sample protein_sequence type: {type(samples[0]['protein_sequence'])}")
    print(f"Sample protein_sequence length: {len(samples[0]['protein_sequence'])}")
    return samples  

def test_processor_alone():
    """æµ‹è¯•Processorå•ç‹¬å·¥ä½œ"""
    print("=== æµ‹è¯•Processorå•ç‹¬å·¥ä½œ ===")
    
    # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
    text_model = "Qwen/Qwen3-1.7B"
    protein_model = "facebook/esm2_t12_35M_UR50D"
    
    text_cached = check_model_cache(text_model, "tokenizer")
    protein_cached = check_model_cache(protein_model, "tokenizer")
    
    print(f"ç¼“å­˜æ£€æŸ¥:")
    print(f"  {text_model}: {'âœ… å·²ç¼“å­˜' if text_cached else 'âŒ æœªç¼“å­˜ï¼Œéœ€è¦ä¸‹è½½'}")
    print(f"  {protein_model}: {'âœ… å·²ç¼“å­˜' if protein_cached else 'âŒ æœªç¼“å­˜ï¼Œéœ€è¦ä¸‹è½½'}")
    
    if not text_cached or not protein_cached:
        print("âš ï¸ éƒ¨åˆ†æ¨¡å‹æœªç¼“å­˜ï¼Œå¯èƒ½é‡åˆ°429é”™è¯¯")
    
    try:
        # 1. å‡†å¤‡tokenizers
        print("åŠ è½½tokenizers...")
        text_tokenizer = AutoTokenizer.from_pretrained(
            text_model, 
            trust_remote_code=True,
            local_files_only=False  # å…è®¸åœ¨çº¿ä¸‹è½½ï¼Œä½†ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
        )
        protein_tokenizer = AutoTokenizer.from_pretrained(
            protein_model,
            local_files_only=False  # å…è®¸åœ¨çº¿ä¸‹è½½ï¼Œä½†ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
        )
        
        # 2. æ·»åŠ ç‰¹æ®Štoken
        text_tokenizer.add_special_tokens({"additional_special_tokens": ["<|protein_pad|>"]})
        
        # 3. åˆ›å»ºProcessor
        processor = ProteinLLMProcessor(
            tokenizer=text_tokenizer,
            protein_tokenizer=protein_tokenizer
        )
        
        # 4. å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = load_test_data()
        
        # æå–æ–‡æœ¬å’Œè›‹ç™½è´¨åºåˆ—
        texts = []
        protein_sequences = []
        
        for sample in test_data:
            # ä½¿ç”¨chat_templateå¤„ç†messages
            text = text_tokenizer.apply_chat_template(
                sample["messages"], 
                tokenize=False, 
                add_generation_prompt=False
            )
            texts.append(text)
            protein_sequences.append(sample["protein_sequence"])  # ç°åœ¨æ˜¯å­—ç¬¦ä¸²
        
        print(f"Sample text: {texts[0][:100]}...")
        print(f"Sample text length: {len(texts[0])}")
        print(f"Sample text length: {len(texts[1])}")
        print(f"Sample text length: {len(texts[2])}")
        print(f"Sample protein: {protein_sequences[0][:20]}...")
        
        # 5. æµ‹è¯•Processor
        result = processor(
            text=texts,
            protein_sequence=protein_sequences,
            max_length_text=512,
            max_length_protein=72,
            return_tensors="pt"
        )
        
        print(f"âœ… ProcessoræˆåŠŸï¼")
        print(f"Result keys: {result.keys()}")
        print(f"Text shape: {result['input_ids'].shape}")
        print(f"Protein shape: {result['protein_tokenized']['input_ids'].shape}")
        print(f"Batch idx map: {result['batch_idx_map']}")
        
        return processor, result
        
    except Exception as e:
        print(f"âŒ Processorå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_datacollator_with_processor():
    """æµ‹è¯•DataCollatorä¸Processorçš„åä½œ"""
    print("\\n=== æµ‹è¯•DataCollatorä¸Processoråä½œ ===")
    
    # æ£€æŸ¥æ¨¡å‹ç¼“å­˜çŠ¶æ€
    text_model = "Qwen/Qwen3-1.7B"
    protein_model = "facebook/esm2_t12_35M_UR50D"
    
    text_model_cached = check_model_cache(text_model, "model")
    protein_model_cached = check_model_cache(protein_model, "model")
    
    print(f"æ¨¡å‹ç¼“å­˜æ£€æŸ¥:")
    print(f"  {text_model}: {'âœ… å·²ç¼“å­˜' if text_model_cached else 'âŒ æœªç¼“å­˜ï¼Œéœ€è¦ä¸‹è½½'}")
    print(f"  {protein_model}: {'âœ… å·²ç¼“å­˜' if protein_model_cached else 'âŒ æœªç¼“å­˜ï¼Œéœ€è¦ä¸‹è½½'}")
    
    if not text_model_cached or not protein_model_cached:
        print("âš ï¸ éƒ¨åˆ†æ¨¡å‹æœªç¼“å­˜ï¼Œå¯èƒ½é‡åˆ°429é”™è¯¯ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
        print("ğŸ’¡ å¯ä»¥å…ˆè¿è¡ŒSFTTraineræµ‹è¯•ï¼Œå®ƒä¼šä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹")
        return None, None
    
    # 1. åˆ›å»ºæ¨¡å‹ï¼ˆä¸ºäº†è·å–processorï¼‰- ä½¿ç”¨ä¸SFTTrainerç›¸åŒçš„é…ç½®
    config = ProteinLLMConfig(
        text_model_name=text_model,
        protein_model_name=protein_model,
        text_model_finetune=True,
        protein_model_finetune=False
    )
    
    try:
        print("åˆ›å»ºæ¨¡å‹ï¼ˆä»ç¼“å­˜åŠ è½½ï¼‰...")
        model = ProteinLLMModel(config=config)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # 2. åˆ›å»ºDataCollator
        data_collator = ProteinLLMDataCollator(
            processor=model.processor,
            max_length_text=512,
            max_length_protein=72
        )
        
        # 3. å‡†å¤‡æ•°æ®
        test_data = load_test_data()
        dataset = Dataset.from_list(test_data)
        
        # 4. æµ‹è¯•DataCollator
        batch_features = [dataset[i] for i in range(5)]  # å–å‰5ä¸ªæ ·æœ¬
        
        batch = data_collator(batch_features)
        
        print(f"âœ… DataCollatoræˆåŠŸï¼")
        print(f"Batch keys: {batch.keys()}")
        print(f"Input IDs shape: {batch['input_ids'].shape}")
        if batch.get('protein_tokenized'):
            print(f"Protein shape: {batch['protein_tokenized']['input_ids'].shape}")
        print(f"Labels shape: {batch['labels'].shape}")
        
        # 5. ç”¨ PyTorch DataLoader æµ‹è¯•â€œä¸¤ä¸ª batchâ€ï¼šbatch_size=5
        print("\n=== ç”¨ DataLoader æµ‹è¯•æ‰¹æ¬¡ï¼ˆbatch_size=5ï¼Œé¢„æœŸ2ä¸ªbatchï¼‰===")
        dl = DataLoader(dataset, batch_size=5, shuffle=False, collate_fn=data_collator)
        num_batches = 0
        for step, b in enumerate(dl):
            num_batches += 1
            print(f"[DataLoader] step={step} -> input_ids {tuple(b['input_ids'].shape)}")
        print(f"DataLoader æ€»æ‰¹æ¬¡æ•°: {num_batches}ï¼ˆåº”ä¸º2ï¼‰")

        return data_collator, batch
        
    except Exception as e:
        print(f"âŒ DataCollatoræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def debug_embedding_fusion():
    """ä¸“é—¨è°ƒè¯•embeddingèåˆé—®é¢˜"""
    print("ğŸ” è°ƒè¯•Embeddingèåˆ...")
    
    config = ProteinLLMConfig(
        text_model_name="Qwen/Qwen3-1.7B",
        protein_model_name="facebook/esm2_t12_35M_UR50D"
    )
    model = ProteinLLMModel(config=config)
    test_data = load_test_data()
    
    data_collator = ProteinLLMDataCollator(
        processor=model.processor,
        max_length_text=512,
        max_length_protein=72
    )
    
    batch = data_collator([test_data[0]])
    
    # ğŸ”§ é€æ­¥è°ƒè¯•
    print("1. æ£€æŸ¥è›‹ç™½è´¨tokenization...")
    protein_tokenized = batch['protein_tokenized']
    print(f"è›‹ç™½è´¨tokens: {protein_tokenized['input_ids'].shape}")
    print(f"è›‹ç™½è´¨tokensèŒƒå›´: {protein_tokenized['input_ids'].min()} - {protein_tokenized['input_ids'].max()}")
    
    print("2. æ£€æŸ¥ESMç¼–ç ...")
    with torch.no_grad():
        protein_embeddings = model.encode_protein_sequences(protein_tokenized)
        if protein_embeddings is not None:
            print(f"è›‹ç™½è´¨embeddings: {protein_embeddings.shape}")
            print(f"è›‹ç™½è´¨embeddingsç»Ÿè®¡: mean={protein_embeddings.mean().item():.4f}, std={protein_embeddings.std().item():.4f}")
            print(f"è›‹ç™½è´¨embeddingsæ˜¯å¦æœ‰NaN: {torch.isnan(protein_embeddings).any()}")
        else:
            print("ğŸš¨ è›‹ç™½è´¨embeddingsä¸ºNone!")
    
    print("3. æ£€æŸ¥æ–‡æœ¬embeddings...")
    text_embeddings = model.text_model.get_input_embeddings()(batch['input_ids'])
    print(f"æ–‡æœ¬embeddings: {text_embeddings.shape}")
    print(f"æ–‡æœ¬embeddingsç»Ÿè®¡: mean={text_embeddings.mean().item():.4f}, std={text_embeddings.std().item():.4f}")
    print(f"æ–‡æœ¬embeddingsæ˜¯å¦æœ‰NaN: {torch.isnan(text_embeddings).any()}")
    
    print("4. ğŸ” å…³é”®æ­¥éª¤ï¼šæ£€æŸ¥fusionè¿‡ç¨‹...")
    print(f"ğŸ”§ æ£€æŸ¥protein_token_id: {model.protein_token_id}")
    protein_positions = (batch['input_ids'] == model.protein_token_id)
    print(f"ğŸ”§ å‘ç°{protein_positions.sum().item()}ä¸ªè›‹ç™½è´¨å ä½ç¬¦ä½ç½®")
    
    if protein_positions.sum().item() > 0:
        print("ğŸ”§ æ£€æŸ¥fusionå‰çš„çŠ¶æ€...")
        print(f"batch['input_ids'].shape: {batch['input_ids'].shape}")
        print(f"batch['attention_mask'].shape: {batch['attention_mask'].shape}")
        print(f"protein_embeddings.shape: {protein_embeddings.shape}")
        fused_embeddings, _ = model.fuse_protein_embeddings(
            batch['input_ids'], 
            batch['attention_mask'], 
            protein_embeddings, 
            batch['batch_idx_map']
        )
        print(f"ğŸ”§ Fusionåembeddingså½¢çŠ¶: {fused_embeddings.shape}")
        print(f"ğŸ”§ Fusionåç»Ÿè®¡: mean={fused_embeddings.mean().item():.4f}, std={fused_embeddings.std().item():.4f}")
        print(f"ğŸ”§ Fusionåæ˜¯å¦æœ‰NaN: {torch.isnan(fused_embeddings).any()}")
        
        if torch.isnan(fused_embeddings).any():
            print("ğŸš¨ NaNåœ¨fusionæ­¥éª¤ä¸­äº§ç”Ÿï¼")
            # æ£€æŸ¥å…·ä½“å“ªäº›ä½ç½®æœ‰NaN
            nan_positions = torch.isnan(fused_embeddings).any(dim=-1)
            print(f"ğŸ”§ NaNä½ç½®æ•°é‡: {nan_positions.sum().item()}")
            print(f"ğŸ”§ NaNä½ç½®ç´¢å¼•: {nan_positions.nonzero().flatten().tolist()}")
        else:
            print("âœ… Fusionæ­¥éª¤æ­£å¸¸")
            
            print("5. ğŸ” æ£€æŸ¥æ¨¡å‹forwardä¸­é—´æ­¥éª¤...")
            # æ‰‹åŠ¨æ‰§è¡Œforwardçš„å„ä¸ªæ­¥éª¤
            model.eval()
            
            # Step 1: èåˆåç›´æ¥ä¼ å…¥text_model
            print("ğŸ”§ Step 1: å°†èåˆembeddingsä¼ å…¥text_model...")
            try:
                text_outputs = model.text_model(
                    inputs_embeds=fused_embeddings,
                    attention_mask=batch['attention_mask'],
                    return_dict=True
                )
                
                print(f"ğŸ”§ Text modelè¾“å‡ºlogitså½¢çŠ¶: {text_outputs.logits.shape}")
                print(f"ğŸ”§ Text model logitsç»Ÿè®¡: mean={text_outputs.logits.mean().item():.4f}, std={text_outputs.logits.std().item():.4f}")
                print(f"ğŸ”§ Text model logitsæ˜¯å¦æœ‰NaN: {torch.isnan(text_outputs.logits).any()}")
                
                if torch.isnan(text_outputs.logits).any():
                    print("ğŸš¨ NaNåœ¨text_model forwardä¸­äº§ç”Ÿï¼")
                else:
                    print("âœ… Text model forwardæ­£å¸¸")
                
            except Exception as e:
                print(f"ğŸš¨ Text model forwardå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
    else:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è›‹ç™½è´¨å ä½ç¬¦ï¼Œå¯èƒ½tokenizationæœ‰é—®é¢˜")

def test_sft_trainer_integration():
    """æµ‹è¯•SFTTraineré›†æˆ - çœŸå®ç‰ˆæœ¬"""
    print("\n=== æµ‹è¯•SFTTraineré›†æˆ - çœŸå®ç‰ˆæœ¬ ===")
    # ...existing import...
    
    # ğŸ”§ ä½¿ç”¨ 10 æ¡æ ·æœ¬ç”¨äºä¸¤ä¸ª batch
    test_data = load_test_data()
    print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆï¼Œå…±{len(test_data)}ä¸ªæ ·æœ¬")
    
    # 1. åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºåŒæ¨¡æ€æ¨¡å‹...")
    config = ProteinLLMConfig(
        text_model_name="Qwen/Qwen3-1.7B",
        protein_model_name="facebook/esm2_t12_35M_UR50D",
        text_model_finetune=True,
        protein_model_finetune=False
    )

    try:
        model = ProteinLLMModel(config=config)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # 2. åˆ›å»ºDataCollator
        data_collator = ProteinLLMDataCollator(
            processor=model.processor,
            max_length_text=512,
            max_length_protein=72
        )
        print(f"âœ… DataCollatoråˆ›å»ºæˆåŠŸ")
        
        # 3. å‡†å¤‡æ•°æ®é›†ï¼ˆ10æ¡ï¼‰
        dataset = Dataset.from_list(test_data)
        print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")

        # 4. é…ç½®è®­ç»ƒå‚æ•°ï¼šbatch=5ï¼Œ1ä¸ªepochï¼ˆåº”åªæœ‰2ä¸ªstepï¼‰
        training_args = TrainingArguments(
            output_dir="./test_sft_output",
            per_device_train_batch_size=5,      # å…³é”®ï¼šæ¯æ‰¹5æ¡
            gradient_accumulation_steps=1,
            num_train_epochs=1,                 # ç”¨epochè€Œä¸æ˜¯max_steps
            learning_rate=1e-5,
            logging_steps=1,
            save_strategy="no",
            eval_strategy="no",
            gradient_checkpointing=True,
            fp16=False,
            bf16=True,
            remove_unused_columns=False,
            dataloader_drop_last=False,         # ä¸ä¸¢å¼ƒï¼Œ10æ ·æœ¬ â†’ 2ä¸ªbatch
            report_to=["wandb"],
            dataloader_num_workers=0,
            ddp_find_unused_parameters=False,
        )
        print(f"âœ… è®­ç»ƒå‚æ•°é…ç½®å®Œæˆï¼ˆbatch=5, epochs=1ï¼‰")
        
        # é‡è¦ï¼šé¿å… SFTTrainer é¢„å¤„ç†è¦†ç›–ä½ çš„ DataCollator
        model.name_or_path = config.text_model_name
        model.config.name_or_path = config.text_model_name

        # 5. åˆ›å»ºSFTTrainer
        print("åˆ›å»ºSFTTrainer...")
        trainer = SFTTrainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
            formatting_func=None,   # ä¿æŒNoneï¼ŒDataCollatorå…¨æƒå¤„ç†
            # ä¸ä¼  processing_class
        )
        print(f"âœ… SFTTraineråˆ›å»ºæˆåŠŸ")
        
        # 6. æ£€æŸ¥ dataloader æ‰¹æ¬¡æ•°ï¼ˆåº”ä¸º2ï¼‰
        print("\nğŸ” è°ƒè¯•Traineræ•°æ®åŠ è½½...")
        train_dataloader = trainer.get_train_dataloader()
        print(f"Debug dataloader length: {len(train_dataloader)}ï¼ˆåº”ä¸º2ï¼‰")
        first_batch = next(iter(train_dataloader))
        print(f"é¦–ä¸ªbatch input_ids shape: {tuple(first_batch['input_ids'].shape)}ï¼ˆåº”ä¸º(5, T)ï¼‰")
        print("ğŸ” Traineræ•°æ®åŠ è½½è°ƒè¯•ç»“æŸ\n")
        
        # 7. å¯é€‰ï¼šè·‘ä¸€ä¸ªéå¸¸çŸ­çš„è®­ç»ƒä»¥éªŒè¯ç®¡é“ï¼ˆä¼šæœ‰2ä¸ªstepï¼‰
        print("\nğŸš€ å¼€å§‹SFTè®­ç»ƒæµ‹è¯•ï¼ˆæœŸæœ›2ä¸ªstepï¼‰...")
        train_result = trainer.train()
        print(f"è®­ç»ƒæŒ‡æ ‡: {train_result.metrics}")
        return trainer, train_result

    except Exception as e:
        print(f"âŒ SFTTraineré›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("ğŸ§¬ æµ‹è¯•é‡æ–°è®¾è®¡çš„åŒå±‚æ¶æ„")
    print("=" * 60)
    
    # é¦–å…ˆæ‰“å°ç¼“å­˜çŠ¶æ€
    print_cache_status()
    
    # æµ‹è¯•Processor
    processor, processor_result = test_processor_alone()
    if processor_result is not None:
        print("âœ… Processoræµ‹è¯•é€šè¿‡\\n")
    
    # æµ‹è¯•DataCollator
    data_collator, batch = test_datacollator_with_processor()
    if batch is not None:
        print("âœ… DataCollatoræµ‹è¯•é€šè¿‡\\n")
    
    debug_embedding_fusion()

    # æµ‹è¯•SFTTraineré›†æˆ - çœŸå®ç‰ˆæœ¬
    trainer, train_result = test_sft_trainer_integration()
    if train_result is not None:
        print("âœ… SFTTraineré›†æˆæµ‹è¯•é€šè¿‡\\n")
    
    print("\\nğŸ‰ å®Œæ•´æ¶æ„æµ‹è¯•å®Œæˆï¼")
    
    # æ€»ç»“æŠ¥å‘Š
    print("\\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"- Processor: {'âœ… é€šè¿‡' if processor_result is not None else 'âŒ å¤±è´¥'}")
    print(f"- DataCollator: {'âœ… é€šè¿‡' if batch is not None else 'âŒ å¤±è´¥'}")
    print(f"- SFTTrainer: {'âœ… é€šè¿‡' if train_result is not None else 'âŒ å¤±è´¥'}")
    
    if all([processor_result is not None, batch is not None, train_result is not None]):
        print("\\nğŸ† æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åŒæ¨¡æ€æ¶æ„å‡†å¤‡å°±ç»ªï¼")
    else:
        print("\\nğŸ’¡ å»ºè®®:")
        if processor_result is None:
            print("- Processorå¤±è´¥ï¼šæ£€æŸ¥tokenizeræ˜¯å¦æ­£ç¡®åŠ è½½")
        if batch is None:
            print("- DataCollatorè·³è¿‡ï¼šç­‰å¾…æ¨¡å‹ç¼“å­˜å®Œæˆåé‡è¯•")
        if train_result is None:
            print("- SFTTrainerå¤±è´¥ï¼šæ£€æŸ¥æ¨¡å‹å’Œè®­ç»ƒé…ç½®")

if __name__ == "__main__":
    main()
