from typing import List, Dict, Any, Optional
import torch
from transformers import DataCollatorForLanguageModeling
from datasets import Dataset

class ProteinLLMDataCollator:
    """
    è›‹ç™½è´¨å¤šæ¨¡æ€æ•°æ®æ”¶é›†å™¨
    
    èŒè´£ï¼š
    1. æ¥æ”¶TRLæ ‡å‡†æ ¼å¼æ•°æ®ï¼ˆåŒ…å«textå’Œprotein_sequenceï¼‰
    2. ä½¿ç”¨ProteinLLMProcessorå¤„ç†åŒæ¨¡æ€æ•°æ®
    3. è¿”å›æ¨¡å‹æœŸæœ›çš„å®Œæ•´batchæ ¼å¼
    
    å·¥ä½œæµç¨‹ï¼š
    TRLæ•°æ® -> æå–è›‹ç™½è´¨åºåˆ— -> Processorå¤„ç† -> æ¨¡å‹è¾“å…¥æ ¼å¼
    """
    
    def __init__(self, processor, tokenizer, max_length_text: int = 512, max_length_protein: int = 100):
        self.processor = processor
        self.tokenizer = tokenizer
        self.max_length_text = max_length_text
        self.max_length_protein = max_length_protein
    
    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """å¤„ç†batchæ•°æ®"""
        print(f"DataCollator received {len(features)} features")
        
        # æå–æ•°æ®
        texts = []
        protein_sequences = []
        
        for i, feature in enumerate(features):
            # æå–æ–‡æœ¬
            if "text" in feature:
                texts.append(feature["text"])
            elif "messages" in feature:
                text = self.tokenizer.apply_chat_template(
                    feature["messages"], 
                    tokenize=False, 
                    add_generation_prompt=False
                )
                texts.append(text)
            else:
                raise ValueError(f"Feature {i} missing 'text' or 'messages' field")
            
            # æå–è›‹ç™½è´¨åºåˆ—
            if "protein_sequence" in feature:
                protein_seq = feature["protein_sequence"]
                if isinstance(protein_seq, list) and len(protein_seq) == 1:
                    protein_seq = protein_seq[0]
                elif isinstance(protein_seq, list):
                    raise ValueError(f"Expected 1 protein sequence per sample, got {len(protein_seq)}")
                protein_sequences.append(protein_seq)
            else:
                print(f"Warning: Feature {i} missing 'protein_sequence', using empty string")
                protein_sequences.append("")
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨Processorå¤„ç†ï¼Œé¿å…å‚æ•°å†²çª
        try:
            batch_protein_sequences = [[seq] for seq in protein_sequences if seq]
            
            if batch_protein_sequences:
                print(f"Processing {len(batch_protein_sequences)} protein sequences")
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¸ä¼ é€’é¢å¤–çš„kwargsï¼Œé¿å…å‚æ•°å†²çª
                batch = self.processor(
                    batch_protein_sequences=batch_protein_sequences,
                    text=texts,
                    max_length_text=self.max_length_text,
                    max_length_protein=self.max_length_protein,
                    return_tensors="pt"
                    # ğŸ”§ ç§»é™¤paddingå’Œtruncationå‚æ•°ï¼Œè®©processorå†…éƒ¨å¤„ç†
                )
            else:
                print("No protein sequences found, using text-only processing")
                # ğŸ”§ çº¯æ–‡æœ¬å¤„ç†ä¹Ÿè¦é¿å…å‚æ•°å†²çª
                batch = self.tokenizer(
                    texts,
                    max_length=self.max_length_text,
                    return_tensors="pt",
                    padding=True,  # åªåœ¨è¿™é‡ŒæŒ‡å®šä¸€æ¬¡
                    truncation=True
                )
                batch["protein_tokenized"] = None
                batch["batch_idx_map"] = []
                
        except Exception as e:
            print(f"Processor error: {e}")
            print("Falling back to text-only processing")
            # ğŸ”§ é™çº§å¤„ç†
            batch = self.tokenizer(
                texts,
                max_length=self.max_length_text,
                return_tensors="pt",
                padding=True,
                truncation=True
            )
            batch["protein_tokenized"] = None
            batch["batch_idx_map"] = []
        
        # æ·»åŠ è®­ç»ƒæ ‡ç­¾
        batch["labels"] = batch["input_ids"].clone()
        
        # ç¡®ä¿batchæ˜¯å­—å…¸æ ¼å¼
        if not isinstance(batch, dict):
            batch = dict(batch)
        
        print(f"DataCollator output keys: {batch.keys()}")
        print(f"Batch input_ids shape: {batch['input_ids'].shape}")
        
        return batch

class ProteinLLMDataCollatorForInference(ProteinLLMDataCollator):
    """æ¨ç†ä¸“ç”¨æ•°æ®æ”¶é›†å™¨ï¼ˆä¸ç”Ÿæˆlabelsï¼‰"""
    
    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        batch = super().__call__(features)
        # ç§»é™¤labelsï¼Œä¿ç•™å…¶ä»–å­—æ®µç”¨äºæ¨ç†
        if "labels" in batch:
            del batch["labels"]
        return batch