from typing import Optional, Dict, Any, List, Union, Tuple
import torch
import torch.nn as nn
from torch.nn import CrossEntropyLoss
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoModelForMaskedLM,
    PreTrainedModel,
    PretrainedConfig
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from .ProteinLLMProcessor import ProteinLLMProcessor


class ProteinLLMModel(PreTrainedModel):
    """
    è›‹ç™½è´¨ä¿¡å·è‚½åˆ†ç±»æ¨¡å‹
    ç»“åˆæ–‡æœ¬LLMå’Œè›‹ç™½è´¨ç¼–ç å™¨ï¼Œä¸“é—¨ç”¨äºä¿¡å·è‚½ç±»å‹è¯†åˆ«ä»»åŠ¡
    
    Architecture:
        - Text Model: Qwen3 (å¤„ç†å¯¹è¯å’Œæ¨ç†)
        - Protein Model: ESM2 (ç¼–ç è›‹ç™½è´¨åºåˆ—)
        - Projection: å°†è›‹ç™½è´¨embeddingæŠ•å½±åˆ°æ–‡æœ¬ç©ºé—´
        - Fusion: åœ¨æ–‡æœ¬ä¸­æ›¿æ¢<|protein_pad|>å ä½ç¬¦
    """
    
    def __init__(
        self,
        config,
        text_model_name: str = "Qwen/Qwen3-1.7B",
        protein_model_name: str = "facebook/esm2_t33_650M_UR50D",
        cache_dir: Optional[str] = None,
        text_model_finetune: bool = True,
        protein_model_finetune: bool = False,  # å†»ç»“è›‹ç™½è´¨æ¨¡å‹ï¼ˆCold Startç­–ç•¥ï¼‰
        **kwargs
    ):
        super().__init__(config)
        
        self.text_model_name = text_model_name
        self.protein_model_name = protein_model_name
        self.text_model_finetune = text_model_finetune
        self.protein_model_finetune = protein_model_finetune

        self.name_or_path = config.text_model_name
        self.config.name_or_path = config.text_model_name
        
        # åŠ è½½æ–‡æœ¬æ¨¡å‹
        print(f"Loading text model: {text_model_name}")
        self.text_model = AutoModelForCausalLM.from_pretrained(
            text_model_name,
            cache_dir=cache_dir,
            trust_remote_code=True,
            # ğŸ”§ ä½¿ç”¨bfloat16å¹³è¡¡ç²¾åº¦å’Œæ˜¾å­˜ï¼Œæ•°å€¼ç¨³å®šæ€§å¥½
            torch_dtype=torch.bfloat16,
            attn_implementation="eager",
            **kwargs
        )
        
        # åŠ è½½æ–‡æœ¬tokenizerå¹¶é…ç½®ç‰¹æ®Štoken
        self.text_tokenizer = AutoTokenizer.from_pretrained(
            text_model_name,
            trust_remote_code=True
        )
        
        self.config.tokenizer = self.text_tokenizer

        # æ·»åŠ è›‹ç™½è´¨ç‰¹æ®Štokenï¼ˆModelçš„èŒè´£ï¼‰
        protein_tokens = ["<|protein_pad|>"]
        num_added = self.text_tokenizer.add_special_tokens({
            "additional_special_tokens": protein_tokens
        })
        
        if num_added > 0:
            # æ‰©å±•æ–‡æœ¬æ¨¡å‹çš„embedding
            self.text_model.resize_token_embeddings(len(self.text_tokenizer))
        
        # è·å–ç‰¹æ®Štoken ID
        self.protein_token_id = self.text_tokenizer.convert_tokens_to_ids("<|protein_pad|>")
        
        # ç¡®ä¿pad_tokenå­˜åœ¨
        if self.text_tokenizer.pad_token is None:
            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token
        
        # åŠ è½½è›‹ç™½è´¨æ¨¡å‹
        print(f"Loading protein model: {protein_model_name}")
        self.protein_model = AutoModelForMaskedLM.from_pretrained(
            protein_model_name,
            cache_dir=cache_dir
        )
        
        # åŠ è½½è›‹ç™½è´¨tokenizer
        self.protein_tokenizer = AutoTokenizer.from_pretrained(protein_model_name)
        
        # åˆ›å»ºæŠ•å½±å±‚ï¼šè›‹ç™½è´¨embedding â†’ æ–‡æœ¬embeddingç©ºé—´
        self.protein_projection = nn.Linear(
            self.protein_model.config.hidden_size,  # ESM2: 1280
            self.text_model.config.hidden_size      # Qwen3-1.7B: 2048
        )
        
        # ğŸ”§ å‚è€ƒBioReasonï¼šæ·»åŠ æŠ•å½±å±‚æƒé‡åˆå§‹åŒ–ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§
        with torch.no_grad():
            # ä½¿ç”¨Xavier uniformåˆå§‹åŒ–ï¼Œå‡å°‘æ•°å€¼ä¸ç¨³å®š
            nn.init.xavier_uniform_(self.protein_projection.weight)
            # biasåˆå§‹åŒ–ä¸º0
            if self.protein_projection.bias is not None:
                nn.init.zeros_(self.protein_projection.bias)
        
        # è®¾ç½®æ¨¡å‹çš„å¯è®­ç»ƒæ€§
        self._set_model_trainability()
        
        # åˆ›å»ºå¤„ç†å™¨ï¼ˆä½¿ç”¨å·²é…ç½®çš„tokenizerï¼‰
        self.processor = ProteinLLMProcessor(
            tokenizer=self.text_tokenizer,
            protein_tokenizer=self.protein_tokenizer
        )
        
        print(f"ProteinLLMModel initialized successfully!")
        print(f"Text vocab size: {len(self.text_tokenizer)}")
        print(f"Protein token ID: {self.protein_token_id}")
    
    def _set_model_trainability(self):
        """è®¾ç½®æ¨¡å‹çš„å¯è®­ç»ƒæ€§ï¼ˆCold Startç­–ç•¥ï¼‰"""
        
        # æ–‡æœ¬æ¨¡å‹çš„å¯è®­ç»ƒæ€§
        for param in self.text_model.parameters():
            param.requires_grad = self.text_model_finetune
        
        # è›‹ç™½è´¨æ¨¡å‹çš„å¯è®­ç»ƒæ€§ï¼ˆé€šå¸¸å†»ç»“ä»¥èŠ‚çœè®¡ç®—ï¼‰
        for param in self.protein_model.parameters():
            param.requires_grad = self.protein_model_finetune
        
        # æŠ•å½±å±‚å§‹ç»ˆå¯è®­ç»ƒ
        for param in self.protein_projection.parameters():
            param.requires_grad = True
        
        print(f"Text model trainable: {self.text_model_finetune}")
        print(f"Protein model trainable: {self.protein_model_finetune}")
        print(f"Projection layer trainable: True")
    
    def encode_protein_sequences(
        self, 
        protein_tokenized: Dict[str, torch.Tensor]
    ) -> torch.Tensor:
        """
        ä½¿ç”¨ESM2è·å–é€æ°¨åŸºé…¸è¡¨ç¤º - ä¿®æ­£ç‰ˆæœ¬
        
        ESM2ç»“æ„ï¼š
        - EsmForMaskedLM.esm (åŸºç¡€ç¼–ç å™¨)
        - EsmForMaskedLM.esm.embeddings
        - EsmForMaskedLM.esm.encoder
        - EsmForMaskedLM.lm_head (MLMå¤´ï¼Œæˆ‘ä»¬ä¸éœ€è¦)
        """
        with torch.set_grad_enabled(self.protein_model_finetune):
            # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨ESM2çš„åŸºç¡€ç¼–ç å™¨è·å–é€æ°¨åŸºé…¸è¡¨ç¤º
            # ç›´æ¥è°ƒç”¨esmç¼–ç å™¨ï¼Œé¿å…MaskedLMçš„å¤æ‚æ€§
            esm_base_model = self.protein_model.esm  # è·å–åŸºç¡€ESMç¼–ç å™¨
            
            # è·å–é€æ°¨åŸºé…¸çš„è¡¨ç¤º
            protein_outputs = esm_base_model(
                input_ids=protein_tokenized["input_ids"],
                attention_mask=protein_tokenized["attention_mask"],
                return_dict=True
            )
            
            # ç°åœ¨å¯ä»¥è·å–last_hidden_stateäº†
            protein_embeddings = protein_outputs.last_hidden_state  # [batch, seq_len, hidden_dim]
            
            # ğŸ”§ ç§»é™¤ESMçš„ç‰¹æ®Štokensï¼š<cls> (pos 0) å’Œ <eos> (pos -1)
            # ESM tokenizeræ ¼å¼ï¼š[<cls>, AA1, AA2, ..., AAn, <eos>]
            # æˆ‘ä»¬åªè¦ä¸­é—´çš„æ°¨åŸºé…¸è¡¨ç¤ºï¼š[AA1, AA2, ..., AAn]
            if protein_embeddings.size(1) > 2:  # ç¡®ä¿åºåˆ—é•¿åº¦è¶³å¤Ÿ
                protein_embeddings = protein_embeddings[:, 1:-1, :]  # ç§»é™¤é¦–å°¾ç‰¹æ®Štokens
            
            # ğŸ”§ å‚è€ƒBioReasonï¼šç¡®ä¿è®¾å¤‡å’Œæ•°æ®ç±»å‹ä¸€è‡´
            protein_embeddings = protein_embeddings.to(
                device=self.protein_projection.weight.device,
                dtype=self.protein_projection.weight.dtype
            )
            
            # åº”ç”¨æŠ•å½±å±‚ï¼šESM hidden_size -> Text hidden_size  
            protein_embeddings = self.protein_projection(protein_embeddings)
            
            return protein_embeddings

    def fuse_protein_embeddings(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        protein_embeddings: torch.Tensor,
        batch_idx_map: List[int]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ğŸ”§ é‡å†™ï¼šå‚è€ƒBioReasonçš„é«˜æ•ˆèåˆæ–¹æ³•
        ä½¿ç”¨å¸ƒå°”æ©ç æ‰¹é‡æ›¿æ¢ï¼Œé¿å…å¾ªç¯å¼•å…¥çš„ä¸ç¨³å®šæ€§
        """
        # è·å–æ–‡æœ¬embeddings
        text_embeddings = self.text_model.get_input_embeddings()(input_ids)
        
        # ğŸ”§ å‚è€ƒBioReasonï¼šç¡®ä¿protein_embeddingsä¸text_embeddingsæ•°æ®ç±»å‹ä¸€è‡´
        if protein_embeddings is not None:
            protein_embeddings = protein_embeddings.to(dtype=text_embeddings.dtype)
        
            # æ‰¾åˆ°æ‰€æœ‰è›‹ç™½è´¨å ä½ç¬¦ä½ç½®
            mask = (input_ids == self.protein_token_id)
            n_protein_tokens = mask.sum().item()
            
            # æ‰å¹³åŒ–æ‰€æœ‰è›‹ç™½è´¨embeddings
            protein_embeds_flat = protein_embeddings.view(-1, protein_embeddings.size(-1))
            n_protein_features = protein_embeds_flat.shape[0]
            
            # ğŸ”§ ä¸¥æ ¼æ£€æŸ¥ï¼šç¡®ä¿æ•°é‡åŒ¹é…
            if n_protein_features != n_protein_tokens:
                raise ValueError(
                    f"Protein features and protein tokens do not match: "
                    f"features {n_protein_features}, tokens: {n_protein_tokens}"
                )
            
            # ğŸ”§ å‚è€ƒBioReasonï¼šä½¿ç”¨å¸ƒå°”æ©ç æ‰¹é‡æ›¿æ¢
            text_embeddings[mask] = protein_embeds_flat
        
        return text_embeddings, attention_mask
    
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        protein_tokenized: Optional[Dict[str, torch.Tensor]] = None,
        batch_idx_map: Optional[List[int]] = None,
        **kwargs
    ) -> CausalLMOutputWithPast:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: æ–‡æœ¬token IDs
            attention_mask: æ–‡æœ¬attention mask  
            labels: è®­ç»ƒæ ‡ç­¾
            protein_tokenized: è›‹ç™½è´¨tokenizationç»“æœ
            batch_idx_map: è›‹ç™½è´¨åˆ°batchçš„æ˜ å°„
            
        Returns:
            CausalLMOutputWithPast: åŒ…å«losså’Œlogits
        """
        
        # 1. ç¼–ç è›‹ç™½è´¨åºåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        protein_embeddings = None
        if protein_tokenized is not None:
            protein_embeddings = self.encode_protein_sequences(protein_tokenized)
        
        # 2. èåˆè›‹ç™½è´¨å’Œæ–‡æœ¬è¡¨ç¤º
        if protein_embeddings is not None and batch_idx_map is not None:
            fused_embeddings, fused_attention_mask = self.fuse_protein_embeddings(
                input_ids=input_ids,
                attention_mask=attention_mask,
                protein_embeddings=protein_embeddings,
                batch_idx_map=batch_idx_map
            )
            
            # ğŸ”§ æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if torch.isnan(fused_embeddings).any():
                raise ValueError("NaN detected in fused_embeddings before text_model forward")
            
            # ğŸ”§ æ·»åŠ æ•°å€¼èŒƒå›´æ£€æŸ¥ï¼Œé˜²æ­¢æå€¼
            if fused_embeddings.abs().max() > 1e6:
                print(f"Warning: Large values in fused_embeddings: max={fused_embeddings.abs().max()}")
            
            # ä½¿ç”¨èåˆåçš„embeddings
            outputs = self.text_model(
                inputs_embeds=fused_embeddings,
                attention_mask=fused_attention_mask,
                labels=labels,
                **kwargs
            )
        else:
            # çº¯æ–‡æœ¬æ¨¡å¼
            outputs = self.text_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                **kwargs
            )
        
        return outputs
    
    def generate(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        protein_tokenized: Optional[Dict[str, torch.Tensor]] = None,
        batch_idx_map: Optional[List[int]] = None,
        max_new_tokens: int = 256,
        do_sample: bool = True,
        temperature: float = 0.7,
        top_p: float = 0.9,
        **kwargs
    ) -> torch.Tensor:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆç”¨äºæ¨ç†ï¼‰
        
        Args:
            input_ids: è¾“å…¥token IDs
            attention_mask: attention mask
            protein_tokenized: è›‹ç™½è´¨æ•°æ®
            batch_idx_map: æ‰¹æ¬¡æ˜ å°„
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
            
        Returns:
            generated_ids: ç”Ÿæˆçš„token IDs
        """
        
        # å¤„ç†è›‹ç™½è´¨embeddings
        if protein_tokenized is not None and batch_idx_map is not None:
            protein_embeddings = self.encode_protein_sequences(protein_tokenized)
            fused_embeddings, fused_attention_mask = self.fuse_protein_embeddings(
                input_ids=input_ids,
                attention_mask=attention_mask,
                protein_embeddings=protein_embeddings,
                batch_idx_map=batch_idx_map
            )
            
            # ä½¿ç”¨èåˆembeddingsç”Ÿæˆ
            generated_ids = self.text_model.generate(
                inputs_embeds=fused_embeddings,
                attention_mask=fused_attention_mask,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_p=top_p,
                pad_token_id=self.text_tokenizer.pad_token_id,
                eos_token_id=self.text_tokenizer.eos_token_id,
                **kwargs
            )
        else:
            # çº¯æ–‡æœ¬ç”Ÿæˆ
            generated_ids = self.text_model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_p=top_p,
                pad_token_id=self.text_tokenizer.pad_token_id,
                eos_token_id=self.text_tokenizer.eos_token_id,
                **kwargs
            )
        
        return generated_ids
    
    # ğŸ”§ æ·»åŠ æ¢¯åº¦æ£€æŸ¥ç‚¹æ”¯æŒæ–¹æ³•
    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):
        """å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹"""
        if hasattr(self.text_model, 'gradient_checkpointing_enable'):
            self.text_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs)
    
    def gradient_checkpointing_disable(self):
        """ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹"""
        if hasattr(self.text_model, 'gradient_checkpointing_disable'):
            self.text_model.gradient_checkpointing_disable()
    
    @property
    def supports_gradient_checkpointing(self):
        """æ£€æŸ¥æ˜¯å¦æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹"""
        return hasattr(self.text_model, 'gradient_checkpointing_enable')
    
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
        """ä»SFTæ£€æŸ¥ç‚¹åŠ è½½ï¼ˆæœ¬åœ°ä¼˜å…ˆï¼‰"""
        # 1) è¯»å–é…ç½®
        config = ProteinLLMConfig.from_pretrained(pretrained_model_name_or_path)
        # 2) æ–‡æœ¬æ¨¡å‹å­ç›®å½•ï¼ˆä¼˜å…ˆæœ¬åœ°ï¼‰
        text_model_dir = os.path.join(pretrained_model_name_or_path, "text_model")
        text_model_name = text_model_dir if os.path.isdir(text_model_dir) else config.text_model_name
        # 3) åˆå§‹åŒ–ï¼ˆä¼šä»æœ¬åœ° text_model_dir åŠ è½½ï¼‰
        model = cls(
            config=config,
            text_model_name=text_model_name,
            protein_model_name=config.protein_model_name,
            **kwargs
        )
        # 4) åŠ è½½æŠ•å½±å±‚
        proj_path = os.path.join(pretrained_model_name_or_path, "protein_projection.pth")
        if os.path.exists(proj_path):
            state = torch.load(proj_path, map_location="cpu")
            model.protein_projection.load_state_dict(state)
        # 5) è¦†ç›– tokenizer/processorï¼ˆè‹¥å·²ä¿å­˜ï¼‰
        tok_dir = os.path.join(pretrained_model_name_or_path, "text_tokenizer")
        if os.path.isdir(tok_dir):
            model.text_tokenizer = AutoTokenizer.from_pretrained(tok_dir, trust_remote_code=True)
        proc_dir = os.path.join(pretrained_model_name_or_path, "processor")
        if os.path.isdir(proc_dir):
            model.processor = ProteinLLMProcessor.from_pretrained(proc_dir)
        return model
    
    def save_pretrained(self, save_directory, **kwargs):
        """ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°æ£€æŸ¥ç‚¹"""
        os.makedirs(save_directory, exist_ok=True)
        # 1) ä¿å­˜config
        self.config.save_pretrained(save_directory)
        # 2) ä¿å­˜æ–‡æœ¬æ¨¡å‹/åˆ†è¯å™¨
        self.text_model.save_pretrained(os.path.join(save_directory, "text_model"), **kwargs)
        self.text_tokenizer.save_pretrained(os.path.join(save_directory, "text_tokenizer"))
        # 3) ä¿å­˜æŠ•å½±å±‚
        torch.save(self.protein_projection.state_dict(), os.path.join(save_directory, "protein_projection.pth"))
        # 4) ä¿å­˜å¤„ç†å™¨
        self.processor.save_pretrained(os.path.join(save_directory, "processor"), **kwargs)


# ç”¨äºé…ç½®çš„ç®€å•configç±»
class ProteinLLMConfig(PretrainedConfig):
    """ProteinLLMé…ç½®ç±»"""
    
    model_type = "protein_llm"
    
    def __init__(
        self,
        text_model_name: str = "Qwen/Qwen3-1.7B",
        protein_model_name: str = "facebook/esm2_t33_650M_UR50D",
        text_model_finetune: bool = True,
        protein_model_finetune: bool = False,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.text_model_name = text_model_name
        self.protein_model_name = protein_model_name
        self.text_model_finetune = text_model_finetune
        self.protein_model_finetune = protein_model_finetune