from typing import Optional, Dict, Any, List, Union, Tuple
import torch
import torch.nn as nn
from torch.nn import CrossEntropyLoss
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoModelForMaskedLM,
    PreTrainedModel,
    PretrainedConfig
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from .ProteinLLMProcessor import ProteinLLMProcessor


class ProteinLLMModel(PreTrainedModel):
    """
    è›‹ç™½è´¨ä¿¡å·è‚½åˆ†ç±»æ¨¡å‹
    ç»“åˆæ–‡æœ¬LLMå’Œè›‹ç™½è´¨ç¼–ç å™¨ï¼Œä¸“é—¨ç”¨äºä¿¡å·è‚½ç±»å‹è¯†åˆ«ä»»åŠ¡
    
    Architecture:
        - Text Model: Qwen2.5 (å¤„ç†å¯¹è¯å’Œæ¨ç†)
        - Protein Model: ESM2 (ç¼–ç è›‹ç™½è´¨åºåˆ—)
        - Projection: å°†è›‹ç™½è´¨embeddingæŠ•å½±åˆ°æ–‡æœ¬ç©ºé—´
        - Fusion: åœ¨æ–‡æœ¬ä¸­æ›¿æ¢<|protein_pad|>å ä½ç¬¦
    """
    
    def __init__(
        self,
        config,
        text_model_name: str = "Qwen/Qwen2.5-Math-7B",
        protein_model_name: str = "facebook/esm2_t33_650M_UR50D",
        cache_dir: Optional[str] = None,
        text_model_finetune: bool = True,
        protein_model_finetune: bool = False,  # å†»ç»“è›‹ç™½è´¨æ¨¡å‹ï¼ˆCold Startç­–ç•¥ï¼‰
        **kwargs
    ):
        super().__init__(config)
        
        self.text_model_name = text_model_name
        self.protein_model_name = protein_model_name
        self.text_model_finetune = text_model_finetune
        self.protein_model_finetune = protein_model_finetune
        
        # åŠ è½½æ–‡æœ¬æ¨¡å‹
        print(f"Loading text model: {text_model_name}")
        self.text_model = AutoModelForCausalLM.from_pretrained(
            text_model_name,
            cache_dir=cache_dir,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            attn_implementation="eager",
            **kwargs
        )
        
        # åŠ è½½æ–‡æœ¬tokenizerå¹¶é…ç½®ç‰¹æ®Štoken
        self.text_tokenizer = AutoTokenizer.from_pretrained(
            text_model_name,
            trust_remote_code=True
        )
        
        self.config.tokenizer = self.text_tokenizer

        # æ·»åŠ è›‹ç™½è´¨ç‰¹æ®Štokenï¼ˆModelçš„èŒè´£ï¼‰
        protein_tokens = ["<|protein_pad|>"]
        num_added = self.text_tokenizer.add_special_tokens({
            "additional_special_tokens": protein_tokens
        })
        
        if num_added > 0:
            # æ‰©å±•æ–‡æœ¬æ¨¡å‹çš„embedding
            self.text_model.resize_token_embeddings(len(self.text_tokenizer))
        
        # è·å–ç‰¹æ®Štoken ID
        self.protein_token_id = self.text_tokenizer.convert_tokens_to_ids("<|protein_pad|>")
        
        # ç¡®ä¿pad_tokenå­˜åœ¨
        if self.text_tokenizer.pad_token is None:
            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token
        
        # åŠ è½½è›‹ç™½è´¨æ¨¡å‹
        print(f"Loading protein model: {protein_model_name}")
        self.protein_model = AutoModelForMaskedLM.from_pretrained(
            protein_model_name,
            cache_dir=cache_dir
        )
        
        # åŠ è½½è›‹ç™½è´¨tokenizer
        self.protein_tokenizer = AutoTokenizer.from_pretrained(protein_model_name)
        
        # åˆ›å»ºæŠ•å½±å±‚ï¼šè›‹ç™½è´¨embedding â†’ æ–‡æœ¬embeddingç©ºé—´
        self.protein_projection = nn.Linear(
            self.protein_model.config.hidden_size,  # ESM2: 1280
            self.text_model.config.hidden_size      # Qwen: 4096
        )
        
        # è®¾ç½®æ¨¡å‹çš„å¯è®­ç»ƒæ€§
        self._set_model_trainability()
        
        # åˆ›å»ºå¤„ç†å™¨ï¼ˆä½¿ç”¨å·²é…ç½®çš„tokenizerï¼‰
        self.processor = ProteinLLMProcessor(
            tokenizer=self.text_tokenizer,
            protein_tokenizer=self.protein_tokenizer
        )
        
        print(f"ProteinLLMModel initialized successfully!")
        print(f"Text vocab size: {len(self.text_tokenizer)}")
        print(f"Protein token ID: {self.protein_token_id}")
    
    def _set_model_trainability(self):
        """è®¾ç½®æ¨¡å‹çš„å¯è®­ç»ƒæ€§ï¼ˆCold Startç­–ç•¥ï¼‰"""
        
        # æ–‡æœ¬æ¨¡å‹çš„å¯è®­ç»ƒæ€§
        for param in self.text_model.parameters():
            param.requires_grad = self.text_model_finetune
        
        # è›‹ç™½è´¨æ¨¡å‹çš„å¯è®­ç»ƒæ€§ï¼ˆé€šå¸¸å†»ç»“ä»¥èŠ‚çœè®¡ç®—ï¼‰
        for param in self.protein_model.parameters():
            param.requires_grad = self.protein_model_finetune
        
        # æŠ•å½±å±‚å§‹ç»ˆå¯è®­ç»ƒ
        for param in self.protein_projection.parameters():
            param.requires_grad = True
        
        print(f"Text model trainable: {self.text_model_finetune}")
        print(f"Protein model trainable: {self.protein_model_finetune}")
        print(f"Projection layer trainable: True")
    
    def encode_protein_sequences(
        self, 
        protein_tokenized: Dict[str, torch.Tensor]
    ) -> torch.Tensor:
        """
        ä½¿ç”¨ESM2è·å–é€æ°¨åŸºé…¸è¡¨ç¤º - ä¿®æ­£ç‰ˆæœ¬
        
        ESM2ç»“æ„ï¼š
        - EsmForMaskedLM.esm (åŸºç¡€ç¼–ç å™¨)
        - EsmForMaskedLM.esm.embeddings
        - EsmForMaskedLM.esm.encoder
        - EsmForMaskedLM.lm_head (MLMå¤´ï¼Œæˆ‘ä»¬ä¸éœ€è¦)
        """
        with torch.set_grad_enabled(self.protein_model_finetune):
            # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨ESM2çš„åŸºç¡€ç¼–ç å™¨è·å–é€æ°¨åŸºé…¸è¡¨ç¤º
            # ç›´æ¥è°ƒç”¨esmç¼–ç å™¨ï¼Œé¿å…MaskedLMçš„å¤æ‚æ€§
            esm_base_model = self.protein_model.esm  # è·å–åŸºç¡€ESMç¼–ç å™¨
            
            # è·å–é€æ°¨åŸºé…¸çš„è¡¨ç¤º
            protein_outputs = esm_base_model(
                input_ids=protein_tokenized["input_ids"],
                attention_mask=protein_tokenized["attention_mask"],
                return_dict=True
            )
            
            # ç°åœ¨å¯ä»¥è·å–last_hidden_stateäº†
            protein_embeddings = protein_outputs.last_hidden_state  # [batch, seq_len, hidden_dim]
            
            # ğŸ”§ ç§»é™¤ESMçš„ç‰¹æ®Štokensï¼š<cls> (pos 0) å’Œ <eos> (pos -1)
            # ESM tokenizeræ ¼å¼ï¼š[<cls>, AA1, AA2, ..., AAn, <eos>]
            # æˆ‘ä»¬åªè¦ä¸­é—´çš„æ°¨åŸºé…¸è¡¨ç¤ºï¼š[AA1, AA2, ..., AAn]
            if protein_embeddings.size(1) > 2:  # ç¡®ä¿åºåˆ—é•¿åº¦è¶³å¤Ÿ
                protein_embeddings = protein_embeddings[:, 1:-1, :]  # ç§»é™¤é¦–å°¾ç‰¹æ®Štokens
            
            # åº”ç”¨æŠ•å½±å±‚ï¼šESM hidden_size -> Text hidden_size
            protein_embeddings = self.protein_projection(protein_embeddings)
            
            return protein_embeddings

    def fuse_protein_embeddings(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        protein_embeddings: torch.Tensor,
        batch_idx_map: List[int]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ä¸¥æ ¼çš„èåˆé€»è¾‘ - å ä½ç¬¦æ•°é‡å¿…é¡»åŒ¹é…æ°¨åŸºé…¸æ•°é‡
        """
        batch_size, text_seq_len = input_ids.shape
        
        # è·å–æ–‡æœ¬embeddings
        text_embeddings = self.text_model.get_input_embeddings()(input_ids)
        fused_embeddings = text_embeddings.clone()
        
        # æ‰¾åˆ°è›‹ç™½è´¨å ä½ç¬¦ä½ç½®
        protein_positions = (input_ids == self.protein_token_id)
        
        if protein_embeddings is not None and protein_positions.any():
            protein_idx = 0
            
            for batch_idx in range(batch_size):
                batch_protein_positions = protein_positions[batch_idx].nonzero(as_tuple=True)[0]
                
                if len(batch_protein_positions) > 0:
                    # ğŸ”§ æ·»åŠ ï¼šæ£€æŸ¥batch_idx_mapèŒƒå›´
                    if protein_idx >= len(batch_idx_map):
                        raise ValueError(
                            f"protein_idx ({protein_idx}) exceeds batch_idx_map length ({len(batch_idx_map)}). "
                            f"This indicates a mismatch between protein sequences and batch samples."
                        )
                    
                    if batch_idx_map[protein_idx] == batch_idx:
                        # å½“å‰æ ·æœ¬çš„è›‹ç™½è´¨embedding
                        protein_embed = protein_embeddings[protein_idx]  # [actual_protein_seq_len, hidden_size]
                        
                        # è®¡ç®—æœ‰æ•ˆæ°¨åŸºé…¸æ•°é‡ï¼ˆå·²ç»ç§»é™¤äº†<cls>å’Œ<eos>ï¼‰
                        valid_mask = protein_embed.norm(dim=-1) > 0
                        valid_aa_count = valid_mask.sum().item()
                        num_placeholders = len(batch_protein_positions)
                        
                        # ğŸ”§ ä¸¥æ ¼æ£€æŸ¥ï¼šå ä½ç¬¦æ•°é‡å¿…é¡»ç­‰äºæ°¨åŸºé…¸æ•°é‡
                        if num_placeholders != valid_aa_count:
                            raise ValueError(
                                f"Mismatch in batch {batch_idx}: "
                                f"Found {num_placeholders} <|protein_pad|> placeholders in text, "
                                f"but protein has {valid_aa_count} valid amino acids. "
                                f"Processor should ensure these numbers match exactly.\n"
                                f"Protein sequence length after removing <cls>/<eos>: {protein_embed.shape[0]}\n"
                                f"Valid amino acids (non-zero norm): {valid_aa_count}\n"
                                f"Placeholder positions: {batch_protein_positions.tolist()}"
                            )
                        
                        if valid_aa_count > 0:
                            valid_aa_embeddings = protein_embed[valid_mask]
                            
                            # ğŸ”§ é€ä¸ªç²¾ç¡®æ›¿æ¢ï¼ˆæ•°é‡å·²ç»éªŒè¯åŒ¹é…ï¼‰
                            for i, pos in enumerate(batch_protein_positions):
                                fused_embeddings[batch_idx, pos] = valid_aa_embeddings[i]
                        
                        protein_idx += 1
        
        return fused_embeddings, attention_mask
    
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        protein_tokenized: Optional[Dict[str, torch.Tensor]] = None,
        batch_idx_map: Optional[List[int]] = None,
        **kwargs
    ) -> CausalLMOutputWithPast:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: æ–‡æœ¬token IDs
            attention_mask: æ–‡æœ¬attention mask  
            labels: è®­ç»ƒæ ‡ç­¾
            protein_tokenized: è›‹ç™½è´¨tokenizationç»“æœ
            batch_idx_map: è›‹ç™½è´¨åˆ°batchçš„æ˜ å°„
            
        Returns:
            CausalLMOutputWithPast: åŒ…å«losså’Œlogits
        """
        
        # 1. ç¼–ç è›‹ç™½è´¨åºåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        protein_embeddings = None
        if protein_tokenized is not None:
            protein_embeddings = self.encode_protein_sequences(protein_tokenized)
        
        # 2. èåˆè›‹ç™½è´¨å’Œæ–‡æœ¬è¡¨ç¤º
        if protein_embeddings is not None and batch_idx_map is not None:
            fused_embeddings, fused_attention_mask = self.fuse_protein_embeddings(
                input_ids=input_ids,
                attention_mask=attention_mask,
                protein_embeddings=protein_embeddings,
                batch_idx_map=batch_idx_map
            )
            
            # ä½¿ç”¨èåˆåçš„embeddings
            outputs = self.text_model(
                inputs_embeds=fused_embeddings,
                attention_mask=fused_attention_mask,
                labels=labels,
                **kwargs
            )
        else:
            # çº¯æ–‡æœ¬æ¨¡å¼
            outputs = self.text_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                **kwargs
            )
        
        return outputs
    
    def generate(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        protein_tokenized: Optional[Dict[str, torch.Tensor]] = None,
        batch_idx_map: Optional[List[int]] = None,
        max_new_tokens: int = 256,
        do_sample: bool = True,
        temperature: float = 0.7,
        top_p: float = 0.9,
        **kwargs
    ) -> torch.Tensor:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆç”¨äºæ¨ç†ï¼‰
        
        Args:
            input_ids: è¾“å…¥token IDs
            attention_mask: attention mask
            protein_tokenized: è›‹ç™½è´¨æ•°æ®
            batch_idx_map: æ‰¹æ¬¡æ˜ å°„
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
            
        Returns:
            generated_ids: ç”Ÿæˆçš„token IDs
        """
        
        # å¤„ç†è›‹ç™½è´¨embeddings
        if protein_tokenized is not None and batch_idx_map is not None:
            protein_embeddings = self.encode_protein_sequences(protein_tokenized)
            fused_embeddings, fused_attention_mask = self.fuse_protein_embeddings(
                input_ids=input_ids,
                attention_mask=attention_mask,
                protein_embeddings=protein_embeddings,
                batch_idx_map=batch_idx_map
            )
            
            # ä½¿ç”¨èåˆembeddingsç”Ÿæˆ
            generated_ids = self.text_model.generate(
                inputs_embeds=fused_embeddings,
                attention_mask=fused_attention_mask,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_p=top_p,
                pad_token_id=self.text_tokenizer.pad_token_id,
                eos_token_id=self.text_tokenizer.eos_token_id,
                **kwargs
            )
        else:
            # çº¯æ–‡æœ¬ç”Ÿæˆ
            generated_ids = self.text_model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_p=top_p,
                pad_token_id=self.text_tokenizer.pad_token_id,
                eos_token_id=self.text_tokenizer.eos_token_id,
                **kwargs
            )
        
        return generated_ids
    
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
        """ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½"""
        # è¿™é‡Œå¯ä»¥å®ç°æ¨¡å‹ä¿å­˜å’ŒåŠ è½½é€»è¾‘
        # ç›®å‰è¿”å›æ–°å®ä¾‹
        config = kwargs.get('config', None)
        return cls(config=config, **kwargs)
    
    def save_pretrained(self, save_directory, **kwargs):
        """ä¿å­˜æ¨¡å‹"""
        # ä¿å­˜æ–‡æœ¬æ¨¡å‹
        self.text_model.save_pretrained(f"{save_directory}/text_model", **kwargs)
        
        # ä¿å­˜æŠ•å½±å±‚
        torch.save(
            self.protein_projection.state_dict(), 
            f"{save_directory}/protein_projection.pth"
        )
        
        # ä¿å­˜å¤„ç†å™¨
        self.processor.save_pretrained(save_directory, **kwargs)


# ç”¨äºé…ç½®çš„ç®€å•configç±»
class ProteinLLMConfig(PretrainedConfig):
    """ProteinLLMé…ç½®ç±»"""
    
    model_type = "protein_llm"
    
    def __init__(
        self,
        text_model_name: str = "Qwen/Qwen2.5-Math-7B",
        protein_model_name: str = "facebook/esm2_t33_650M_UR50D",
        text_model_finetune: bool = True,
        protein_model_finetune: bool = False,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.text_model_name = text_model_name
        self.protein_model_name = protein_model_name
        self.text_model_finetune = text_model_finetune
        self.protein_model_finetune = protein_model_finetune