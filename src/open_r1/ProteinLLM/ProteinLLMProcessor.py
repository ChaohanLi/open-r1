from typing import List, Optional, Union, Dict, Any
import torch
from transformers import ProcessorMixin, BatchFeature
from transformers.tokenization_utils_base import TextInput


class ProteinLLMProcessor(ProcessorMixin):
    """
    è›‹ç™½è´¨å¤šæ¨¡æ€å¤„ç†å™¨ - éµå¾ªHuggingFaceè®¾è®¡æ¨¡å¼
    
    èŒè´£ï¼š
    1. ä½¿ç”¨ä¸¤ç§tokenizeråˆ†åˆ«å¤„ç†åŒæ¨¡æ€æ•°æ®
    2. è®¡ç®—å ä½ç¬¦æ•°é‡å¹¶åœ¨æ–‡æœ¬ä¸­æŒ–å‘
    3. è¿”å›å¤„ç†åçš„å•æ ·æœ¬ç»“æœ
    
    æ³¨æ„ï¼šä¸è´Ÿè´£batchæ‰“åŒ…ï¼Œé‚£æ˜¯DataCollatorçš„èŒè´£
    """
    
    attributes = ["tokenizer", "protein_tokenizer"]
    valid_kwargs = ["model"]
    tokenizer_class = ("Qwen2Tokenizer", "Qwen2TokenizerFast")
    protein_tokenizer_class = ("EsmTokenizer",)
    
    def __init__(self, tokenizer=None, protein_tokenizer=None, **kwargs):
        self.tokenizer = tokenizer
        self.protein_tokenizer = protein_tokenizer
        self.protein_token = "<|protein_pad|>"
        
        super().__init__(tokenizer, protein_tokenizer)
        
        # ç¡®ä¿pad_tokenå­˜åœ¨
        if not hasattr(self.tokenizer, 'pad_token') or self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    # ğŸ”§ æ–°å¢ï¼šTRLå…¼å®¹æ€§å±æ€§
    @property
    def pad_token(self):
        """TRLæœŸæœ›çš„pad_tokenå±æ€§"""
        return self.tokenizer.pad_token
    
    @property
    def eos_token(self):
        """TRLæœŸæœ›çš„eos_tokenå±æ€§"""
        return self.tokenizer.eos_token
    
    @property
    def pad_token_id(self):
        """TRLæœŸæœ›çš„pad_token_idå±æ€§"""
        return self.tokenizer.pad_token_id
    
    @property
    def eos_token_id(self):
        """TRLæœŸæœ›çš„eos_token_idå±æ€§"""
        return self.tokenizer.eos_token_id

    @property
    def bos_token(self):
        """TRLæœŸæœ›çš„bos_tokenå±æ€§"""
        return getattr(self.tokenizer, 'bos_token', None)
    
    @property
    def bos_token_id(self):
        """TRLæœŸæœ›çš„bos_token_idå±æ€§"""
        return getattr(self.tokenizer, 'bos_token_id', None)
    
    # ğŸ”§ TRLæœŸæœ›çš„æ–¹æ³• - å§”æ‰˜ç»™å†…éƒ¨tokenizer
    def apply_chat_template(self, *args, **kwargs):
        """å§”æ‰˜ç»™å†…éƒ¨tokenizer"""
        return self.tokenizer.apply_chat_template(*args, **kwargs)
    
    def convert_tokens_to_ids(self, *args, **kwargs):
        """å§”æ‰˜ç»™å†…éƒ¨tokenizer - TRLéœ€è¦æ­¤æ–¹æ³•"""
        return self.tokenizer.convert_tokens_to_ids(*args, **kwargs)
    
    def convert_ids_to_tokens(self, *args, **kwargs):
        """å§”æ‰˜ç»™å†…éƒ¨tokenizer"""
        return self.tokenizer.convert_ids_to_tokens(*args, **kwargs)
    
    def encode(self, *args, **kwargs):
        """å§”æ‰˜ç»™å†…éƒ¨tokenizer"""
        return self.tokenizer.encode(*args, **kwargs)
    
    def __len__(self):
        """è¿”å›tokenizerçš„è¯æ±‡è¡¨å¤§å°"""
        return len(self.tokenizer)

    def __call__(
        self,
        text: Union[str, List[str]] = None,
        protein_sequence: Union[str, List[str]] = None,
        max_length_text: int = 1024,
        max_length_protein: int = 128,
        return_tensors: Optional[str] = None,
        **kwargs,
    ) -> BatchFeature:
        """
        å¤„ç†åŒæ¨¡æ€æ•°æ® - è›‹ç™½è´¨åºåˆ—æ˜¯å¿…éœ€çš„
        
        Args:
            text: æ–‡æœ¬æ•°æ®ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
            protein_sequence: è›‹ç™½è´¨åºåˆ—ï¼ˆå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰- å¿…éœ€
            max_length_text: æ–‡æœ¬æœ€å¤§é•¿åº¦
            max_length_protein: è›‹ç™½è´¨æœ€å¤§é•¿åº¦
            return_tensors: è¿”å›å¼ é‡æ ¼å¼
            
        Returns:
            BatchFeature: åŒ…å«å¤„ç†åçš„åŒæ¨¡æ€æ•°æ®
        """
        
        # ä¸¥æ ¼åŒæ¨¡æ€è¦æ±‚
        if text is None or protein_sequence is None:
            raise ValueError("Both text and protein_sequence must be provided for dual-modal processing")
        
        # 1. æ ‡å‡†åŒ–è¾“å…¥æ ¼å¼
        if isinstance(text, str):
            text = [text]
        
        if isinstance(protein_sequence, str):
            protein_sequence = [protein_sequence]
        
        if len(text) != len(protein_sequence):
            raise ValueError(f"Text and protein sequence counts must match: {len(text)} vs {len(protein_sequence)}")
        
        batch_size = len(text)
        
        # 2. å¤„ç†è›‹ç™½è´¨åºåˆ—ï¼ˆESM tokenizeræ¥å—å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
        print(f"Processing {len(protein_sequence)} protein sequences")
        
        protein_tokenized = self.protein_tokenizer(
            protein_sequence,  # ç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²åˆ—è¡¨
            padding=True,
            truncation=True,
            max_length=max_length_protein,
            return_tensors=return_tensors,
            return_attention_mask=True,
        )
        
        # 3. ä¸ºæ¯ä¸ªæ–‡æœ¬æ ·æœ¬æŒ–å‘
        processed_texts = []
        for i, txt in enumerate(text):
            if self.protein_token in txt:
                # è®¡ç®—è¯¥æ ·æœ¬çš„å®é™…æ°¨åŸºé…¸tokenæ•°
                attention_mask = protein_tokenized['attention_mask'][i]
                total_tokens = attention_mask.sum().item()
                # ESMæ ¼å¼ï¼š[<cls>, AA1, AA2, ..., AAn, <eos>] â†’ æ°¨åŸºé…¸æ•° = total - 2
                amino_acid_count = max(1, total_tokens - 2)
                
                # æŒ–å‘ï¼šå°†1ä¸ªå ä½ç¬¦æ‰©å±•ä¸ºå¯¹åº”æ•°é‡çš„å ä½ç¬¦
                expanded_text = txt.replace(
                    self.protein_token, 
                    self.protein_token * amino_acid_count
                )
                processed_texts.append(expanded_text)
                
                print(f"Sample {i}: {total_tokens} total tokens â†’ {amino_acid_count} amino acid placeholders")
            else:
                processed_texts.append(txt)
        
        # 4. å¤„ç†æ–‡æœ¬ï¼ˆä½¿ç”¨æŒ–å‘åçš„æ–‡æœ¬ï¼‰
        text_tokenized = self.tokenizer(
            processed_texts,
            padding=True,
            truncation=True,
            max_length=max_length_text,
            return_tensors=return_tensors,
            **kwargs
        )
        
        # 5. ç»„è£…ç»“æœ
        result = {
            # æ–‡æœ¬tokenizationç»“æœ
            **text_tokenized,
            # è›‹ç™½è´¨tokenizationç»“æœ
            "protein_tokenized": {
                "input_ids": protein_tokenized["input_ids"],
                "attention_mask": protein_tokenized["attention_mask"],
            },
            # ç®€åŒ–çš„æ‰¹æ¬¡æ˜ å°„
            "batch_idx_map": list(range(batch_size)),
        }
        
        print(f"Processor output - Text: {text_tokenized['input_ids'].shape}, Protein: {protein_tokenized['input_ids'].shape}")
        
        return BatchFeature(data=result)
    
    def batch_decode(self, *args, **kwargs) -> List[str]:
        return self.tokenizer.batch_decode(*args, **kwargs)
    
    def decode(self, *args, **kwargs) -> str:
        return self.tokenizer.decode(*args, **kwargs)
    
    @property
    def model_input_names(self) -> List[str]:
        tokenizer_names = self.tokenizer.model_input_names
        protein_names = ["protein_tokenized", "batch_idx_map"]
        return list(dict.fromkeys(tokenizer_names + protein_names))