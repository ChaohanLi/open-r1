from typing import List, Optional, Union, Dict, Any
import torch
from transformers import ProcessorMixin, BatchFeature
from transformers.tokenization_utils_base import TextInput


class ProteinLLMProcessor(ProcessorMixin):
    """
    ç®€åŒ–çš„è›‹ç™½è´¨å¤„ç†å™¨ - ä¸“é—¨é’ˆå¯¹ä¿¡å·è‚½åˆ†ç±»ä»»åŠ¡
    å‡è®¾ï¼šæ¯ä¸ªæ ·æœ¬ç¡®å®šåŒ…å«1ä¸ªé•¿åº¦70çš„è›‹ç™½è´¨åºåˆ—
    """
    
    attributes = ["tokenizer", "protein_tokenizer"]
    valid_kwargs = ["model"]
    tokenizer_class = ("Qwen2Tokenizer", "Qwen2TokenizerFast")
    protein_tokenizer_class = ("EsmTokenizer",)
    
    def __init__(self, tokenizer=None, protein_tokenizer=None, **kwargs):
        self.tokenizer = tokenizer
        self.protein_tokenizer = protein_tokenizer
        self.protein_token = "<|protein_pad|>"
        
        super().__init__(tokenizer, protein_tokenizer)
        
        # ç¡®ä¿pad_tokenå­˜åœ¨
        if not hasattr(self.tokenizer, 'pad_token') or self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    # ğŸ”§ æ–°å¢ï¼šTRLå…¼å®¹æ€§å±æ€§
    @property
    def pad_token(self):
        """TRLæœŸæœ›çš„pad_tokenå±æ€§"""
        return self.tokenizer.pad_token
    
    @property
    def eos_token(self):
        """TRLæœŸæœ›çš„eos_tokenå±æ€§"""
        return self.tokenizer.eos_token
    
    @property
    def pad_token_id(self):
        """TRLæœŸæœ›çš„pad_token_idå±æ€§"""
        return self.tokenizer.pad_token_id
    
    @property
    def eos_token_id(self):
        """TRLæœŸæœ›çš„eos_token_idå±æ€§"""
        return self.tokenizer.eos_token_id

    @property
    def bos_token(self):
        """TRLæœŸæœ›çš„bos_tokenå±æ€§"""
        return getattr(self.tokenizer, 'bos_token', None)
    
    @property
    def bos_token_id(self):
        """TRLæœŸæœ›çš„bos_token_idå±æ€§"""
        return getattr(self.tokenizer, 'bos_token_id', None)
    
    # ğŸ”§ TRLæœŸæœ›çš„æ–¹æ³• - å§”æ‰˜ç»™å†…éƒ¨tokenizer
    def apply_chat_template(self, *args, **kwargs):
        """å§”æ‰˜ç»™å†…éƒ¨tokenizer"""
        return self.tokenizer.apply_chat_template(*args, **kwargs)
    
    def convert_tokens_to_ids(self, *args, **kwargs):
        """å§”æ‰˜ç»™å†…éƒ¨tokenizer - TRLéœ€è¦æ­¤æ–¹æ³•"""
        return self.tokenizer.convert_tokens_to_ids(*args, **kwargs)
    
    def convert_ids_to_tokens(self, *args, **kwargs):
        """å§”æ‰˜ç»™å†…éƒ¨tokenizer"""
        return self.tokenizer.convert_ids_to_tokens(*args, **kwargs)
    
    def encode(self, *args, **kwargs):
        """å§”æ‰˜ç»™å†…éƒ¨tokenizer"""
        return self.tokenizer.encode(*args, **kwargs)
    
    def __len__(self):
        """è¿”å›tokenizerçš„è¯æ±‡è¡¨å¤§å°"""
        return len(self.tokenizer)

    def __call__(
        self,
        batch_protein_sequences: List[List[str]],
        text: Union[str, List[str]],
        max_length_text: int = 1024,
        max_length_protein: int = 128,
        return_tensors: str = "pt",
        **kwargs,
    ) -> BatchFeature:
        """
        ç®€åŒ–çš„å¤„ç†æ–¹æ³• - ä¿®å¤ESM tokenè®¡ç®—
        """
        
        # ç¡®ä¿textæ˜¯åˆ—è¡¨
        if isinstance(text, str):
            text = [text]
        
        batch_size = len(text)
        
        # ç®€åŒ–ï¼šç›´æ¥æå–è›‹ç™½è´¨åºåˆ—ï¼ˆæ¯ä¸ªæ ·æœ¬1ä¸ªï¼‰
        protein_sequences = []
        for sequences in batch_protein_sequences:
            assert len(sequences) == 1, f"Expected 1 protein per sample, got {len(sequences)}"
            protein_sequences.append(sequences[0])
        
        # å¤„ç†è›‹ç™½è´¨åºåˆ—
        if protein_sequences:
            protein_tokenized = self.protein_tokenizer(
                protein_sequences,
                padding=True,
                truncation=True,
                max_length=max_length_protein,
                return_tensors=return_tensors,
                return_attention_mask=True,
            )
            
            # ç®€åŒ–çš„å ä½ç¬¦æ›¿æ¢ï¼šä¸€å¯¹ä¸€æ˜ å°„
            processed_text = []
            for i, txt in enumerate(text):
                if self.protein_token in txt:
                    # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—è›‹ç™½è´¨tokenæ•°é‡ï¼ˆæ’é™¤<cls>å’Œ<eos>ï¼‰
                    attention_mask = protein_tokenized['attention_mask'][i]
                    total_tokens = attention_mask.sum().item()
                    
                    # ESMä¼šæ·»åŠ <cls>å’Œ<eos>ï¼Œæ‰€ä»¥å®é™…è›‹ç™½è´¨tokenæ•° = total - 2
                    protein_token_count = max(1, total_tokens - 2)  # è‡³å°‘ä¿ç•™1ä¸ªtoken
                    
                    # ç®€å•æ›¿æ¢ï¼šä¸€ä¸ªå ä½ç¬¦å˜æˆå¤šä¸ª
                    processed_txt = txt.replace(
                        self.protein_token, 
                        self.protein_token * protein_token_count
                    )
                    processed_text.append(processed_txt)
                else:
                    processed_text.append(txt)
            
            text = processed_text
        else:
            protein_tokenized = None
        
        # å¤„ç†æ–‡æœ¬
        text_inputs = self.tokenizer(
            text,
            padding=True,
            truncation=True,
            max_length=max_length_text,
            return_tensors=return_tensors,
            **kwargs
        )
        
        # ç»„è£…ç»“æœ
        result = {**text_inputs}
        if protein_tokenized is not None:
            result["protein_tokenized"] = protein_tokenized
            # ç®€åŒ–çš„batch_idx_mapï¼š[0, 1, 2, ...]
            result["batch_idx_map"] = list(range(batch_size))
        
        return BatchFeature(data=result)
    
    def batch_decode(self, *args, **kwargs) -> List[str]:
        return self.tokenizer.batch_decode(*args, **kwargs)
    
    def decode(self, *args, **kwargs) -> str:
        return self.tokenizer.decode(*args, **kwargs)
    
    @property
    def model_input_names(self) -> List[str]:
        tokenizer_names = self.tokenizer.model_input_names
        protein_names = ["protein_tokenized", "batch_idx_map"]
        return list(dict.fromkeys(tokenizer_names + protein_names))